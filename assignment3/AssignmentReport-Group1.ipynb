{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "First we calculate the number of zero padding so the convolved image will b HxW = 3x5\n",
    "\n",
    "From stanfords cs231n’s introduction we know that:\n",
    "$$W_2 = (W_1 -F + 2P)/s + 1 = 5-3 + 2P/1 + 1 = 2+P$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$H_2 = (H_1 - F + 2P)/S+1 = 3-3+2P/1+1 = P$$\n",
    "\n",
    "Therefore we get P = 1.\n",
    "\n",
    "The spatial convolution by hand is shown in tha image below:\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"task1a1.png\" width=\"300\"/>\n",
    "<img src=\"task1a2.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "Max pooling.\n",
    "\n",
    "The main reason Max pooling is commonly imbibed into Convolutional Neural Network is to progressively\n",
    "reduce the spatial size to reduce the number of parameters. Max pooling creates a lower resolution version of\n",
    "the input containing the large or important structural elements without the fine detailed that is not useful.\n",
    "This makes the image more robust to changes in the input.\n",
    "\n",
    "## task 1c)\n",
    "From stanfords cs231n’s introduction we know that:\n",
    "\n",
    "$$W_2 = (W_1-F+2P)/S+1$$\n",
    "\n",
    "Since the stride is equal to one the width/height will not make a difference.\n",
    "This can be se in the equation below. P is equal to Padding , S is stride, W is width\n",
    "and F is the filter size.\n",
    "\n",
    "$$W_2 = (W_1 -F +2P)/S+1$$\n",
    "\n",
    "$$W_2= W_1 = W$$\n",
    "\n",
    "$$W = (W-F-2P)/S+1$$\n",
    "\n",
    "$$W(S-1) = -F+2P+S$$\n",
    "\n",
    "$$(F+W(S+1)-S)/2 = P$$\n",
    "\n",
    "$$F=5, S=1$$\n",
    "\n",
    "$$P = (5+W(1-1)-1)/2 = 2$$\n",
    "\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "As described in task 1c we know that:\n",
    "\n",
    "$$W_2 = (W_1-F+2P)/S+1$$\n",
    "\n",
    "To find the spatial dimensions of these kernels we calculate $F$\n",
    "\n",
    "$$F = S(1-W_2) + W_1 + 2P = 1 \\cdot (1-504) + 512 + 2 \\cdot 0 = 9  $$\n",
    "\n",
    "The image is square, therefore we know that the Kernal is square.\n",
    "The filter size will be 9x9\n",
    "\n",
    "## task 1e)\n",
    "Since we have no padding the spatial dimensions of the pooled feature maps in the first pooling\n",
    "layer is given by:\n",
    "\n",
    "$$W_2 = (W_1 -F + 2P)/S + 1 = (504 - 2)/2 +1 = 252$$\n",
    "\n",
    "since it is square we get the dimensions of the first pooling layer equal 252x252\n",
    "\n",
    "## task 1f)\n",
    "As in task 1e we have that:\n",
    "\n",
    "$$W_2 = (W_1 -F + 2P)/S + 1$$\n",
    "\n",
    "$$= (504-2)/2 + 1 = 252$$\n",
    "\n",
    "\n",
    "since it is squared the feature maps in the second layer is 250x250\n",
    "\n",
    "## task 1g)\n",
    "\n",
    "\n",
    "\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "\n",
    "![](plots/task2.png)\n",
    "\n",
    "### Task 2b)\n",
    "\n",
    "| Model  |   Training accuracy | Testing accuracy | Validation accuracy| \n",
    "| :-: | :-: | :-: | :-: | \n",
    "| 1 | 0.8920 | 0.7431 | 0.7486 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "We improved the model step by step to see the impact of different actions. Adding dropout between each convolution layer with probaby of 0.2 after all max-pool layers resulted in less overfitting. The next thing was adding batch normalization between every layer which resulted in faster training. For the first model, we also added another layer.\n",
    "\n",
    "\n",
    "Finally, we increased the learning rate by a factor of 10 from $10^{-2}$ to $10^{-1}$. This greatly impacted the training speed. It did, however, make the training less stable.\n",
    "\n",
    "At this point, the test accuracy had increased from around 70% to close to 75%.\n",
    "\n",
    "We then decreased the learning rate again to $3 \\cdot 10^{-2}$, as trial and error deemed this the best.\n",
    "\n",
    "In the next model, we removed the extra layer. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Model 1:\n",
    "\n",
    "| Layer | Layer Type | Filters/Hidden units| Activation|\n",
    "| :- | -: | :-: | :-: | \n",
    "| 1 | Conv2D | 16 | ReLU | \n",
    "| 1 | Conv2D | 32 | ReLU | \n",
    "| 1 | MaxPool2D | - | -| \n",
    "| 1 | Dropout | - | -| \n",
    "| 1 | BatchNorm | - | -|\n",
    "| 2 | Conv2D | 64 | ReLU | \n",
    "| 2 | MaxPool2D | - | -| \n",
    "| 2 | Dropout | - | -| \n",
    "| 2 | BatchNorm | - | -|\n",
    "| 3 | Conv2D | 128 | ReLU | \n",
    "| 3 | MaxPool2D | - | -| \n",
    "| 3 | Dropout | - | -| \n",
    "| 3 | BatchNorm | - | -|\n",
    "| 4 | Conv2D | 256 | ReLU | \n",
    "| 4 | MaxPool2D | - | -| \n",
    "| 4 | Dropout | - | -| \n",
    "| 4 | BatchNorm | - | - | \n",
    "| - | Flatten | - | -| \n",
    "| 5 | FC | 64 | ReLU |\n",
    "| 5 | BatchNorm | - | -| \n",
    "| 6 | FC | 10 | SoftMax | \n",
    "\n",
    "\n",
    "#### Model 2:\n",
    "\n",
    "| Layer      | Layer Type   | Filters/Hidden units  | Activation    |\n",
    "| :- | -: | :-: | :-: | \n",
    "| 1 | Conv2D | 16 | ReLU | \n",
    "| 1 | Conv2D | 32 | ReLu |\n",
    "| 1 | MaxPool2D | - | -| \n",
    "| 1 | Dropout | - | -|\n",
    "| 1 | BatchNorm | - | -|\n",
    "| 2 | Conv2D | 64 | ReLu | \n",
    "| 2 | MaxPool2D | - | -| \n",
    "| 2 | Dropout | - | -| \n",
    "| 2 | BatchNorm | - | -|\n",
    "| 3 | Conv2D | 128 | ReLu | \n",
    "| 3 | MaxPool2D | - | -| \n",
    "| 3 | Dropout | - | -| \n",
    "| 3 | BatchNorm | - | - | \n",
    "| - | Flatten | - | -| \n",
    "| 4 | FC | 64 | ReLU | \n",
    "| 4 | BatchNorm | - | ReLU | \n",
    "| 5 | FC | 10 | SoftMax | \n",
    "\n",
    "\n",
    "#### Other hyperparameters\n",
    "Both models were trained with the specifications given at the top of this section. \n",
    "These include dropout with probability of 0.2 for all layers. Batch size remained at 64 for all the tests and for both networks, and the learning rate at $3*10^{-2}$ for both models. All convolution layers ended up with a kernel size of (3,3) and padding of size 1 which seemed to work better than (5,5) with padding of size 2.\n",
    "\n",
    "The mean and standard deviation was also changed to mean=$[0.485, 0.456, 0.406]$ and std=$[0.229, 0.224, 0.225]$ as recommended for this dataset in task 4.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 3b)\n",
    "\n",
    "| Model  |   Training accuracy | Testing accuracy | Validation accuracy| Training loss|\n",
    "| :-: | :-: | :-: | :-: | \n",
    "| 1 | 0.8705 | 0.7987 | 0.8040 | 0.3774|\n",
    "| 2 | 0.8552 | 0.7772 | 0.7853 | 0.4157|\n",
    "\n",
    "\n",
    "The plots for model 1 (the deepest and best model) can be found below : \n",
    "\n",
    "![](plots/task3c.png)\n",
    " \n",
    "### Task 3c)\n",
    "\n",
    "We saw the greatest improvement with dropout, resulting in much less overfitting. This is not surprising, as dropout is frequently used as a way to prevent overfitting.\n",
    "\n",
    "Training speed was greatly affected by learning rate (not surprisingly).\n",
    "\n",
    "We ended up not using data augmentation. The dataset is seemingly both quite large and quite balanced, and we therefore focused on other things. This could be implemented as a next-step.\n",
    "\n",
    "Increasing the depth of the network with smaller kernels (3,3 instead of 5,5) increased the performance. This could have to do with more details being picked up by the smaller kernels.\n",
    "\n",
    "### Task 3d)\n",
    "![](plots/task3d.png)\n",
    "### Task 3e)\n",
    "The best model was so close to 80% that we accepted it as 80% simply beacuse we do not have time to retrain and retrain the network with different parameters any longer.\n",
    "### Task 3f)\n",
    "There is not much overfitting in out best model as seen by the plot in 3b) compared to task 2. Yet, the training accuracy is significantly higher than our validation - and test accuracy, which signifies that the model is still overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "![](plots/task4a.png)\n",
    "\n",
    "Batch size: 32\n",
    "\n",
    "Learning rate: $5 \\cdot 10^{-4}$\n",
    "\n",
    "Data augmentation: None\n",
    "\n",
    "Resize: 224\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "\n",
    "\n",
    "Final test accuracy: 0.8865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "\n",
    "Passing the image through the first filter yields the following images for the selected indices:\n",
    "\n",
    "![](plots/filtering.png)\n",
    "\n",
    "It seems the first two and the third are essentially edge detectors while the others seem to detect certain colors. Each activation will have different properties like this, and together they extract different features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "After a bit of playing around, we ended up with the following output which is the first 10 activations from the last convolution layer (after the last batch normalization which we were told was fine):\n",
    "\n",
    "![](plots/4c.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}