{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "We know that:\n",
    "\n",
    "\n",
    "$$w_{ji}:=w_{ji} - \\alpha \\frac{\\partial C}{\\partial w_{ij}}$$\n",
    "\n",
    "From the chain rule we also know that:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{ij}} = \\frac{\\partial C}{\\partial z_{j}}* \\frac{\\partial z_j}{\\partial w_{ji}}$$\n",
    "\n",
    "$$\\delta_j  \\frac{\\partial z_j}{\\partial w_{ij}} = \\delta_j * \\frac{\\partial (\\sum_{i=1}^{I} w_{ij}x_i)}{\\partial w_{ji}}$$\n",
    "\n",
    "$$=\\delta_j x_i$$\n",
    "\n",
    "Therefore we get that:\n",
    "\n",
    "\n",
    "$$w{ij} = w_{ij} - \\alpha \\delta_j x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "First we will solve the question with one hidden layer before making it general:\n",
    "\n",
    "The picture below shows a given network with one hidden layer:\n",
    "\n",
    "![](1neural.png)\n",
    "\n",
    "From the image we get that:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_0} = \\frac{\\partial a_0}{\\partial z_0}*\\frac{\\partial z_1}{\\partial a_0} * \\frac{\\partial a_1}{\\partial z_1} * \\frac{\\partial C}{\\partial a_1}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_1} = \\frac{\\partial a_1}{\\partial z_1} * \\frac{\\partial C}{\\partial a_1} = \\delta_k$$\n",
    "\n",
    "$$\\frac{\\partial a_0}{\\partial z_0} = \\frac{\\partial (\\sigma(z_0))}{\\partial z_0} = f'(z_0)$$\n",
    "\n",
    "$$\\frac{\\partial z_1}{\\partial a_0} = \\frac{\\partial (a_0 w)}{\\partial a_0} = w$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_0} = \\delta_k f(z_0) w$$\n",
    "\n",
    "Converting to a more general form we get:\n",
    "\n",
    "$$\\delta_j = f'(z_j)* \\sum_{k} w_{kj} \\delta_k$$\n"
   ]
  },
  {
   "source": [
    "# Task 1 b)\n",
    "\n",
    "In the calculation above we can move backwards. \n",
    "\n",
    "Consider 100 images in one batch.\n",
    "\n",
    "In the (last) layer, we have 100x10 as output. The derivative of the cost function $-(y-\\hat{y})$ over 100 images is therefore 10x100.\n",
    "\n",
    "The last gradient will be $-\\frac{1}{100} a_{1}^T \\cdot \\delta_k$ which is 64x100@100x10 which is 64x10\n",
    "\n",
    "Moving backwards, $d_j = f'(z_0)*\\cdot\\delta_k\\cdot w_{1}^T$ is a 100x64*(100x10@10x64) operation which yields 100x64.\n",
    "\n",
    "Now that we have $\\delta_j$, we can multiply this by X and we get that the first gradient is $\\frac{1}{100}\\cdot X^T\\cdot \\delta_j$ which is a 785x100@x100x64 which yields 785x64"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "\n",
    "Notice how the training accuracy quickly outperforms the validation accuracy (and similarly for the training - and validation loss). This is most likely due to the fact that the extra hidden layers allows the network to pick up nonlinear relationships in the training set that do not apply to the general mnist image (as seen by the gap between training and validation).\n",
    "\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "The number of parameters in the network is:\n",
    "\n",
    "$$(28*28+1)*64+64*10 = 50880$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, since we have it, here is an image of shuffled vs unshuffled performance of the network:\n",
    "\n",
    "![](task3.png)\n",
    "\n",
    "As requested, the different tricks of the trade were added incrementally. First, the improved sigmoid, then improved weights and finally momentum. The momentum was implemented as Equation (6) in the assignment text in this plot. Since the performance actually dropped, and the gap between validation and training loss increased using momentum, we deactivated this feature for the remaining parts of this exercise.\n",
    "\n",
    "![](task3_2_momentum(2).png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a and b)\n",
    "\n",
    "Training the network with the optimal hyperparameters from Task 3, it seems more hidden units yield better accuracy. It is however, more computationally expensive to use more hidden units and this performance increase will converge. Finding the optimal amount of hidden units is an optimization problem that will not be discussed further in this exercise.\n",
    "\n",
    "![](task4_a_b.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "The plot below was used training a network of dimension (32,32,10) with $(28*28+1)*32+32*32+32*10 = 26464$ parameters. \n",
    "In the next image (Task 4e), a network is trained with dimension (48,48,10) which yields 40464 parameters and a better accuracy. This plot was kept as a demonstration that this network of roughly half the parameters does not perform as well as the (48,48,10) network. This network also seems to suffer from overfitting, and early stopping does not kick in early enough to prevent the validation loss to increase.\n",
    "![](task4d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "Adding many layers does not seem to have any positive effects. The training does, however, become more unstable (at one test run the accuracy dropped drastically to less than 20 $\\%$ - we have not managed to reproduce and save an image of this.) Training time does, however, increase with the amount of layers (amount of parameters). The 2 hidden layer network with 48 units in each hidden layer performed quite similarly to the network with one hidden layer of 64 units. \n",
    "\n",
    "![](task4_e.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (TDT4265)",
   "language": "python",
   "name": "pycharm-59ecf39d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}