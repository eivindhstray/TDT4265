{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## task 1a)\n",
    "\n",
    "$$\\def\\doubleunderline#1{\\underline{\\underline{#1}}}$$\n",
    "\n",
    "We have given:\n",
    "\n",
    "$$C^n =-((y^n\\ln{\\hat{y}^n})+(1-y^n)\\ln{(1-\\hat{y}^n)})$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\\frac{\\partial f(x^n)}{\\partial w_i} = x^n_if(x^n)(1-f(x^n))$$\n",
    "\n",
    "\n",
    "$$\\hat{y}^n= f(x^n)$$\n",
    "\n",
    "we start by calculating $\\frac{\\partial C^n(w)}{\\partial f(x^n)}$:\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial f(x^n)} = \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} = \\frac{\\hat{y}^n-y^n}{(1-\\hat{y}^n)\\hat{y}^n}$$\n",
    "\n",
    "From the chain rule we know that:\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\partial C^n(w)}{\\partial f(x^n)} * \\frac{\\partial f(x^n)}{\\partial w_i}$$\n",
    "\n",
    "Therefore we have that\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\hat{y}^n-y^n}{(1-\\hat{y}^n)\\hat{y}^n} * x^n_if(x^n)(1-f(x^n)) $$\n",
    "\n",
    "since $\\hat{y}= f(x)$ we get:\n",
    "\n",
    "$$\\doubleunderline{\\frac{\\partial C^n(w)}{\\partial w_i} = -(y^n  - \\hat{y}^n)x^n_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "$$\\def\\doubleunderline#1{\\underline{\\underline{#1}}}$$\n",
    "\n",
    "$$\\hat{y}_k = \\frac{e^{z_k}}{\\sum_{k'}^{K} e^{z_k{'}} }$$\n",
    "\n",
    "we start with the derivation of  $\\hat{y}_k$, where $\\hat{y}_k$ represent the probability that x is a member of class k.\n",
    "\n",
    "\n",
    "We will break the derivation of the softmax into two stages, namely $k= k'$ and $k \\neq k'$:\n",
    "\n",
    "$$k= k':$$ \n",
    "\n",
    "$$\\frac{\\partial \\hat{y}_k}{\\partial z_k} = \\frac{e^{z_k} *\\sum_{k'}^{K} e^{z_k{'}}-e^{z_i}*e^{z_k}}{(\\sum_{k'}^{K} e^{z_k{'}})^2}$$\n",
    "\n",
    "\n",
    "$$= \\frac{e^{z_k}}{\\sum_{k'}^{K} e^{z_k{'}}} *\\frac{\\sum_{k'}^{K} e^{z_k{'}}- e^{z_k}}{\\sum_{k'}^{K} e^{z_k{'}}} $$\n",
    "\n",
    "$$=\\hat{y}_k(1-\\hat{y}_k)$$\n",
    "\n",
    "$$k \\neq k':$$ \n",
    "\n",
    "$$\\frac{\\partial \\hat{y}_{k}}{\\partial z_{k'}} = \\frac{0-e^{z_k}e^{z_k{'}}}{(\\sum_{k'}^{K} e^{z_k{'}})^2}$$\n",
    "\n",
    "$$ = -\\hat{y}_k \\hat{y}_{k'}$$\n",
    "\n",
    "We can now use these to simplify our equation when derivating the cross entropy.\n",
    "\n",
    "The cross-entropy cost function for multiple classes is defined as:\n",
    "\n",
    "$$ C(w) =\\frac{1}{N} \\sum_{n=1}^{N} C^n(w)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$C^n(w) =-\\sum_{k=1}^{K} y^n_k \\ln{\\hat{y}^n_k} $$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial z_{k}} = - \\frac{\\partial y_k}{\\partial z_{k}} * \\frac{y_k}{\\hat{y}_k} - \\sum_{k \\neq k'}^{K} \\frac{y_{k'}}{\\hat{y}_k}*\\frac{\\partial y_k}{\\partial z_{k'}}$$\n",
    "\n",
    "\n",
    "We simplify the equation with our result from the derivative of softmax:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial z_{k}} = -y_k + \\hat{y}_k  \\sum_{k = 1}^{K} y_k$$\n",
    "\n",
    "$$=-y_k + \\hat{y}_k$$\n",
    "\n",
    "\n",
    "We are given:\n",
    "\n",
    "$$z_k = \\sum_{i}^{I} w_{k,i} * x_i$$\n",
    "\n",
    "We then get: \n",
    "\n",
    "\n",
    "$$\\frac{\\partial z_k}{\\partial w_{k,i}} = x_i$$\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_{k,i}} = \\frac{\\partial z_k}{\\partial w_{k,i}} * \\frac{\\partial C^n(w)}{\\partial z_{k}}$$\n",
    "\n",
    "\n",
    "$$ = x_i * (-y_k + \\hat{y}_k)$$\n",
    "\n",
    "$$= \\doubleunderline{x^n_i(y^n_k-\\hat{y}^n_k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2d)\n",
    "\n",
    "In the un-shuffled case, the training stops after 34 epochs using early stopping. The plots are displayed below\n",
    "\n",
    "![](task2d_binary_train_accuracy.png)\n",
    "\n",
    "![](task2d_binary_train_loss.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)\n",
    "\n",
    "Shuffling the gives less \"spikes\". In the binary case, if 2/10 images have label 1 and the remaining 8/10 have the label 0 which is the case with this dataset then classifying 0 will generally hava a high successrate. This may be the reason that the unshuffled case gives periodic drops in accuracy and corresponding periodic spikes in the cost function. The spikes do become smaller and smaller as training proceeds, so this wrongful naive guessing occurs less frequently as the model is trained.\n",
    "\n",
    "When shuffling the dataset, these periodic spikes disappear, as the pattern is no longer predictable. Using this technique, the different labels occur randomly, making the curve more \"Gaussian\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "\n",
    "From the accuracy plot, it seems that the training accuracy outperforms the validation accuracy after roughly 3000 training steps. Notice how the training accuracy keeps improving while the validation accuracy remains more or less constant (it is still increasing but only marginally). \n",
    "\n",
    "Seeing this tendency of overfitting is less clear from the training - and validation loss, but it can arguably be said that the training loss outperforms the validation loss in this case as well. \n",
    "\n",
    "There are tendencies of overfitting, but in this case overfitting is not an issue. Furthermore, since we are using Early Stopping from task 2 during training in this task as well, the training stops before overfitting becomes too visible. If training continued, however, the validation loss could actually converge or even start increasing while the training loss keeps decreasing. This is not the case here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "$$J(w) = C(w) + \\lambda * R(w)$$\n",
    "\n",
    "$$\\frac{\\partial J(w)}{\\partial w} = \\frac{\\partial C(w)}{\\partial w} + \\lambda * \\frac{\\partial R(w)}{\\partial w}$$\n",
    "\n",
    "\n",
    "$$ = -1/N * \\sum_{n=1}^{N} -x^n_j(y^n_k-\\hat{y}^n_k) + \\lambda 2 w $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "$\\lambda = 1$ appears clearer than $\\lambda = 0$. \n",
    "\n",
    "$\\lambda = 0 $:\n",
    "\n",
    "![](task4b_softmax_weight_0.png)\n",
    "\n",
    "$\\lambda = 1$:\n",
    "\n",
    "![](task4b_softmax_weight_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "Regularization is a way of bounding the weights by adding a penalty in the cost function that increases with a higher $L_2-Norm$ of the weights. This forces the optimal point of the cost function away from the original minimum. The higher the $\\lambda$, the \"harder\" the weights are pulled away from a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (TDT4265)",
   "language": "python",
   "name": "pycharm-59ecf39d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}