{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## task 1a)\n",
    "\n",
    "$$\\def\\doubleunderline#1{\\underline{\\underline{#1}}}$$\n",
    "\n",
    "We have given:\n",
    "\n",
    "$$C^n =-((y^n\\ln{\\hat{y}^n})+(1-y^n)\\ln{(1-\\hat{y}^n)})$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\\frac{\\partial f(x^n)}{\\partial w_i} = x^n_if(x^n)(1-f(x^n))$$\n",
    "\n",
    "\n",
    "$$\\hat{y}^n= f(x^n)$$\n",
    "\n",
    "we start by calculating $$\\frac{\\partial C^n(w)}{\\partial f(x^n)}$$ :\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial f(x^n)} = \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} = \\frac{\\hat{y}^n-y^n}{(1-\\hat{y}^n)\\hat{y}^n}$$\n",
    "\n",
    "From the chain rule we know that:\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\partial C^n(w)}{\\partial f(x^n)} * \\frac{\\partial f(x^n)}{\\partial w_i}$$\n",
    "\n",
    "Therefore we have that\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\hat{y}^n-y^n}{(1-\\hat{y}^n)\\hat{y}^n} * x^n_if(x^n)(1-f(x^n)) $$\n",
    "\n",
    "since $$\\hat{y}= f(x)$$ we get:\n",
    "\n",
    "$$\\doubleunderline{\\frac{\\partial C^n(w)}{\\partial w_i} = -(y^n  - \\hat{y}^n)x^n_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "$$\\def\\doubleunderline#1{\\underline{\\underline{#1}}}$$\n",
    "\n",
    "$$\\hat{y}_k = \\frac{e^{z_k}}{\\sum_{k'}^{K} e^{z_k{'}} }$$\n",
    "\n",
    "we start with the derivation of  $$\\hat{y}_k$$, where $$\\hat{y}_k$$ represent the probability that x is a member of class k.\n",
    "\n",
    "\n",
    "We will break the derivation of the softmax into two stages, namely $$k= k'$$ and $$k \\neq k'$$:\n",
    "\n",
    "$$k= k'$$ :\n",
    "\n",
    "$$\\frac{\\partial \\hat{y}_k}{\\partial z_k} = \\frac{e^{z_k} *\\sum_{k'}^{K} e^{z_k{'}}-e^{z_i}*e^{z_k}}{(\\sum_{k'}^{K} e^{z_k{'}})^2}$$\n",
    "\n",
    "\n",
    "$$= \\frac{e^{z_k}}{\\sum_{k'}^{K} e^{z_k{'}}} *\\frac{\\sum_{k'}^{K} e^{z_k{'}}- e^{z_k}}{\\sum_{k'}^{K} e^{z_k{'}}} $$\n",
    "\n",
    "= $$\\hat{y}_k(1-\\hat{y}_k)$$\n",
    "\n",
    "$$k \\neq k'$$ :\n",
    "\n",
    "$$\\frac{\\partial \\hat{y}_{k}}{\\partial z_{k'}} = \\frac{0-e^{z_k}e^{z_k{'}}}{(\\sum_{k'}^{K} e^{z_k{'}})^2}$$\n",
    "\n",
    "$$ = -\\hat{y}_k \\hat{y}_{k'}$$\n",
    "\n",
    "We can now use these to simplify our equation when derivating the cross entropy:\n",
    "\n",
    "The cross-entropy cost function for multiple classes is defined as:\n",
    "\n",
    "$$ C(w) =\\frac{1}{N} \\sum_{n=1}^{N} C^n(w)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$C^n(w) =-\\sum_{k=1}^{K} y^n_k \\ln{\\hat{y}^n_k} $$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial z_{k}} = - \\frac{\\partial y_k}{\\partial z_{k}} * \\frac{y_k}{\\hat{y}_k} - \\sum_{k \\neq k'}^{K} \\frac{y_{k'}}{\\hat{y}_k}*\\frac{\\partial y_k}{\\partial z_{k'}}$$\n",
    "\n",
    "\n",
    "We simplify the equation with our result from the derivative of softmax:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial z_{k}} = -y_k + \\hat{y}_k  \\sum_{k = 1}^{K} y_k$$\n",
    "\n",
    "$$=-y_k + \\hat{y}_k$$\n",
    "\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "We are given:\n",
    "\n",
    "$$z_k = \\sum_{i}^{I} w_{k,i} * x_i$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial z_k}{\\partial w_{k,i}} = x_i$$\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_{k,i}} = \\frac{\\partial z_k}{\\partial w_{k,i}} * \\frac{\\partial C^n(w)}{\\partial z_{k}}$$\n",
    "\n",
    "\n",
    "$$ = x_i * (-y_k + \\hat{y}_k)$$\n",
    "\n",
    "$$= \\doubleunderline{x^n_i(y^n_k-\\hat{y}^n_k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "In the un-shuffled case, the training stops after 122 epochs using early stopping. From 2e), it is clear that shuffling the dataset yields a later stop at epoch 138 but an overall more stable training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)\n",
    "\n",
    "Shuffling the gives less \"spikes\". In the binary case, if 2/10 images have label 1 and the remaining 8/10 have the label 0 which is the case with this dataset then classifying 0 will generally hava a high successrate. This may be the reason that the unshuffled case gives periodic drops in accuracy and corresponding periodic spikes in the cost function. The spikes do become smaller and smaller as training proceeds, so this wrongful naive guessing occurs less frequently.\n",
    "\n",
    "When shuffling the dataset, these periodic spikes disappear, as the pattern is no longer predictable. Using this technique, the different labels occur randomly, making the curve more \"Gaussian\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)\n",
    "\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "\n",
    "From the accuracy plot, it seems that the training accuracy outperforms the validation accuracy after roughly 3000 training steps. Notice how the training accuracy keeps improving while the validation remains more or less constant (it is still increasing but only marginally)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Fill in image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4b_softmax_weight_0.png)\n",
    "\n",
    "![](task4b_softmax_weight_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-59ecf39d",
   "language": "python",
   "display_name": "PyCharm (TDT4265)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}